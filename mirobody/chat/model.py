import logging

from typing import Any, Union, Optional, Literal
from typing_extensions import NotRequired, TypedDict
from pydantic import model_validator, BaseModel, Field

from langchain_core.messages.base import (
    BaseMessage,
)
from langchain_core.messages import (
    AIMessageChunk,
)
from langchain_core.messages.tool import (
    InvalidToolCall,
    ToolCall,
    default_tool_chunk_parser,
    default_tool_parser,
)
from langchain_core.messages.tool import (
    invalid_tool_call as create_invalid_tool_call,
)
from langchain_core.messages.tool import (
    tool_call as create_tool_call,
)
from langchain_core.messages.tool import (
    tool_call_chunk as create_tool_call_chunk,
)

#-----------------------------------------------------------------------------

class InputTokenDetails(TypedDict, total=False):
    """Breakdown of input token counts.

    Does *not* need to sum to full input token count. Does *not* need to have all keys.

    Example:

        .. code-block:: python

            {
                "audio": 10,
                "cache_creation": 200,
                "cache_read": 100,
            }

    .. versionadded:: 0.3.9

    May also hold extra provider-specific keys.

    """

    audio: int
    """Audio input tokens."""
    cache_creation: int
    """Input tokens that were cached and there was a cache miss.

    Since there was a cache miss, the cache was created from these tokens.
    """
    cache_read: int
    """Input tokens that were cached and there was a cache hit.

    Since there was a cache hit, the tokens were read from the cache. More precisely,
    the model state given these tokens was read from the cache.
    """

#-----------------------------------------------------------------------------

class OutputTokenDetails(TypedDict, total=False):
    """Breakdown of output token counts.

    Does *not* need to sum to full output token count. Does *not* need to have all keys.

    Example:

        .. code-block:: python

            {
                "audio": 10,
                "reasoning": 200,
            }

    .. versionadded:: 0.3.9

    """

    audio: int
    """Audio output tokens."""
    reasoning: int
    """Reasoning output tokens.

    Tokens generated by the model in a chain of thought process (i.e. by OpenAI's o1
    models) that are not returned as part of model output.
    """

#-----------------------------------------------------------------------------

class UsageMetadata(TypedDict):
    """Usage metadata for a message, such as token counts.

    This is a standard representation of token usage that is consistent across models.

    Example:

        .. code-block:: python

            {
                "input_tokens": 350,
                "output_tokens": 240,
                "total_tokens": 590,
                "input_token_details": {
                    "audio": 10,
                    "cache_creation": 200,
                    "cache_read": 100,
                },
                "output_token_details": {
                    "audio": 10,
                    "reasoning": 200,
                },
            }

    .. versionchanged:: 0.3.9

        Added ``input_token_details`` and ``output_token_details``.

    """

    input_tokens: int
    """Count of input (or prompt) tokens. Sum of all input token types."""
    output_tokens: int
    """Count of output (or completion) tokens. Sum of all output token types."""
    total_tokens: int
    """Total token count. Sum of input_tokens + output_tokens."""
    input_token_details: NotRequired[InputTokenDetails]
    """Breakdown of input token counts.

    Does *not* need to sum to full input token count. Does *not* need to have all keys.
    """
    output_token_details: NotRequired[OutputTokenDetails]
    """Breakdown of output token counts.

    Does *not* need to sum to full output token count. Does *not* need to have all keys.
    """

#-----------------------------------------------------------------------------

class AIMessage(BaseMessage):
    """Message from an AI.

    AIMessage is returned from a chat model as a response to a prompt.

    This message represents the output of the model and consists of both
    the raw output as returned by the model together standardized fields
    (e.g., tool calls, usage metadata) added by the LangChain framework.
    """

    example: bool = False
    """Use to denote that a message is part of an example conversation.

    At the moment, this is ignored by most models. Usage is discouraged.
    """

    tool_calls: list[ToolCall] = []
    """If provided, tool calls associated with the message."""
    invalid_tool_calls: list[InvalidToolCall] = []
    """If provided, tool calls with parsing errors associated with the message."""
    usage_metadata: Optional[UsageMetadata] = None
    """If provided, usage metadata for a message, such as token counts.

    This is a standard representation of token usage that is consistent across models.
    """

    type: Literal["ai"] = "ai"
    """The type of the message (used for deserialization). Defaults to "ai"."""

    def __init__(
        self, content: Union[str, list[Union[str, dict]]], **kwargs: Any
    ) -> None:
        """Pass in content as positional arg.

        Args:
            content: The content of the message.
            kwargs: Additional arguments to pass to the parent class.
        """
        super().__init__(content=content, **kwargs)

    @property
    def lc_attributes(self) -> dict:
        """Attrs to be serialized even if they are derived from other init args."""
        return {
            "tool_calls": self.tool_calls,
            "invalid_tool_calls": self.invalid_tool_calls,
        }

    # TODO: remove this logic if possible, reducing breaking nature of changes
    @model_validator(mode="before")
    @classmethod
    def _backwards_compat_tool_calls(cls, values: dict) -> Any:
        check_additional_kwargs = not any(
            values.get(k)
            for k in ("tool_calls", "invalid_tool_calls", "tool_call_chunks")
        )
        if check_additional_kwargs and (
            raw_tool_calls := values.get("additional_kwargs", {}).get("tool_calls")
        ):
            try:
                if issubclass(cls, AIMessageChunk):
                    values["tool_call_chunks"] = default_tool_chunk_parser(
                        raw_tool_calls
                    )
                else:
                    parsed_tool_calls, parsed_invalid_tool_calls = default_tool_parser(
                        raw_tool_calls
                    )
                    values["tool_calls"] = parsed_tool_calls
                    values["invalid_tool_calls"] = parsed_invalid_tool_calls
            except Exception as e:
                logging.debug(f"Failed to parse tool calls: {str(e)}", exc_info=True)

        # Ensure "type" is properly set on all tool call-like dicts.
        if tool_calls := values.get("tool_calls"):
            values["tool_calls"] = [
                create_tool_call(**{k: v for k, v in tc.items() if k != "type"})
                for tc in tool_calls
            ]
        if invalid_tool_calls := values.get("invalid_tool_calls"):
            values["invalid_tool_calls"] = [
                create_invalid_tool_call(**{k: v for k, v in tc.items() if k != "type"})
                for tc in invalid_tool_calls
            ]

        if tool_call_chunks := values.get("tool_call_chunks"):
            values["tool_call_chunks"] = [
                create_tool_call_chunk(**{k: v for k, v in tc.items() if k != "type"})
                for tc in tool_call_chunks
            ]

        return values

#-----------------------------------------------------------------------------

class ChatFileObject:
    def __init__(
        self,
        file_key    : str = "",
        file_type   : str = "",
        file_name   : str = "",
        file_url    : str = "",
        file_size   : int = 0,

        duration    : int = 0,  # For audio/video files, in millisecond.
        storage_key : str = "",
        url         : str = ""
    ):
        self.file_key   = file_key
        self.file_type  = file_type
        self.file_name  = file_name
        self.file_url   = file_url
        self.file_size  = file_size

        self.duration   = duration
        self.storage_key= storage_key
        self.url        = url

#-----------------------------------------------------------------------------

class ChatStreamRequest:
    """
    Chat stream request from frontend.
    
    All parameters should be explicitly passed from the API request,
    avoiding implicit context dependencies for thread safety and testability.
    """
    
    def __init__(
        self,
        question            : str = "",
        query_user_id       : str = "",
        agent               : str = "",
        provider            : str = "",
        enable_mcp          : int = 1,

        session_id          : str = "",
        question_id         : str = "",
        trace_id            : str = "",
        reference_task_id   : str = "",

        file_list           : list[ChatFileObject] | None = None,
        prompt_name         : str = "",

        user_id             : str = "",
        user_name           : str = "",
        group_id            : str = "",
        
        token               : str = "",
        language            : str = "",
        timezone            : str = ""
    ):
        self.question       = question
        self.query_user_id  = query_user_id
        self.agent          = agent
        self.provider       = provider
        self.enable_mcp     = enable_mcp

        self.session_id     = session_id
        self.question_id    = question_id
        self.msg_id         = question_id
        self.trace_id       = trace_id
        self.reference_task_id = reference_task_id

        self.file_list      = file_list
        self.prompt_name    = prompt_name

        self.user_id        = user_id
        self.user_name      = user_name
        self.group_id       = group_id
        
        self.token          = token
        self.language       = language
        self.timezone       = timezone

#-----------------------------------------------------------------------------

class UserInfo(BaseModel):
    user_id: str = Field(..., description="User ID")
    user_name: str = Field(..., description="User Name")

#-----------------------------------------------------------------------------

class File(BaseModel):
    name: str = Field(..., description="Name of the file")
    file_path: str = Field(..., description="File path of the file")
    file_type: str = Field(..., description="File type of the file")

#-----------------------------------------------------------------------------

class Image(BaseModel):
    name: str = Field(..., description="Name of the image")
    image_path: str = Field(..., description="Image path of the image")
    image_type: Literal["jpg", "png", "jpeg"] = Field(
        ..., description="Image type of the image"
    )

#-----------------------------------------------------------------------------

class ChatMessage(BaseModel):
    type: Literal["user", "ai"] = Field(..., description="Role of the message")
    content: str = Field("", description="Content of the message")
    file: Optional[File] = Field(None, description="File of the message")
    image: Optional[Image] = Field(None, description="Image of the message")

#-----------------------------------------------------------------------------

class ChatRequest(BaseModel):
    session_id: str = Field(..., description="Session ID")
    user: UserInfo = Field(..., description="User Information")
    timestamp: int = Field(..., description="Timestamp of the request")
    timezone: str = Field(..., description="Timezone of the request")
    language: str = Field(..., description="Language of the request")
    messages: list[ChatMessage] = Field(
        ..., description="List of previous interactions"
    )

#-----------------------------------------------------------------------------
